[package]
name = "llama_cpp_sys"
version = "0.3.2"
description = "Automatically-generated bindings to llama.cpp's C API"
edition = "2021"
authors = ["Dakota Thompson <me@scriptis.net>", "Pedro Valente <pedro.amaral.valente@gmail.com>"]
repository = "https://github.com/edgenai/llama_cpp-rs"
license = "MIT OR Apache-2.0"
readme = "../../README.md"
exclude = ["thirdparty/**/tests", "thirdparty/**/scripts", "thirdparty/**/prompts", "thirdparty/**/gguf-py", "thirdparty/**/.devops", "thirdparty/**/docs", "thirdparty/**/media"]
publish = true
links = "llama"

[dependencies]
ash = { version = "0.37.3", default-features = false, features = ["linked"], optional = true }
cudarc = { version = "0.10.0", features = ["cublaslt"], optional = true }
link-cplusplus = "1.0.9"

[build-dependencies]
bindgen = "0.69.4"
cc = { version = "1.0.83", features = ["parallel"] }
once_cell = "1.19.0"

[features]
default = ["compat", "native"]
compat = [] # this feature modifies the symbols exposed by the generated libraries to avoid conflicts
native = ["avx", "avx2", "fma", "f16c", "accel"]
avx = []
avx2 = []
avx512 = []
avx512_vmbi = []
avx512_vnni = []
fma = []
f16c = [] # implied when compiled using MSVC with avx2/avx512
accel = [] # Accelerate framework
mpi = []
cuda = ["dep:cudarc"]
cuda_f16 = ["cuda"]
cuda_dmmv = ["cuda"] # use dmmv instead of mmvq CUDA kernels
cuda_mmq = ["cuda"] # use mmq kernels instead of cuBLAS
metal = []
blas = []
hipblas = []
clblast = []
vulkan = ["dep:ash"]
